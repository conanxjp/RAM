{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import config as cf\n",
    "from preprocess import (processData, prepareData)\n",
    "import time\n",
    "from tensorflow.python.ops import math_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing data for model ...\n",
      "encoding data ...\n"
     ]
    }
   ],
   "source": [
    "# print('preprocessing data ...')\n",
    "# processData('2014', 'rest', 'glove');\n",
    "print('preparing data for model ...')\n",
    "trainData, testData, validData, sampleData = prepareData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_iterations = 500\n",
    "batch_size = 32\n",
    "full_iterations = 50\n",
    "learning_rate = 0.005\n",
    "reg_eta = 0.001\n",
    "dim_lstm = 300\n",
    "num_AL = 3\n",
    "\n",
    "\n",
    "# dimensionalities\n",
    "dim_word = 300\n",
    "dim_sentence = 80\n",
    "dim_polarity = 3\n",
    "\n",
    "train_word_weights = np.load(cf.FOLDER + 'word_positions_train.npy')\n",
    "test_word_weights = np.load(cf.FOLDER + 'word_positions_test.npy')\n",
    "valid_word_weights = np.load(cf.FOLDER + 'word_positions_valid.npy')\n",
    "sample_word_weights = np.load(cf.FOLDER + 'word_positions_sample.npy')\n",
    "glove = np.load(cf.FOLDER + 'glove.npy')\n",
    "timestamp = str(int(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp, valid_aspects_encoding = validData\n",
    "valid_X, valid_y, valid_seqlen = temp\n",
    "valid_X = np.array(valid_X)\n",
    "valid_seqlen = np.array(valid_seqlen)\n",
    "temp, sample_aspects_encoding = sampleData\n",
    "sample_X, sample_y, sample_seqlen = temp\n",
    "temp, train_aspects_encoding = trainData\n",
    "train_X, train_y, train_seqlen = temp\n",
    "train_X = np.array(train_X)\n",
    "train_seqlen = np.array(train_seqlen)\n",
    "temp, test_aspects_encoding = testData\n",
    "test_X, test_y, test_seqlen = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.int32, [None, dim_sentence])\n",
    "aspects = tf.placeholder(tf.float32, [None, dim_word])\n",
    "seqlen = tf.placeholder(tf.int32, None)\n",
    "sentence_locs = tf.placeholder(tf.float32, [None, dim_sentence])\n",
    "y = tf.placeholder(tf.int32, [None, dim_polarity])\n",
    "# dropout_keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "#     inputs = tf.nn.embedding_lookup(glove, X)\n",
    "#     inputs = tf.cast(inputs, tf.float32)\n",
    "#     inputs = tf.nn.dropout(inputs, keep_prob=dropout_keep_prob)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('attention', reuse = tf.AUTO_REUSE):\n",
    "    Wal = tf.get_variable(\n",
    "        name='W_al',\n",
    "        shape=[num_AL, 1, dim_lstm * 3 + dim_word + 1],\n",
    "#         initializer=tf.random_uniform_initializer(-0.003, 0.003),\n",
    "        initializer=tf.contrib.layers.xavier_initializer(),\n",
    "        regularizer=tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )\n",
    "    Bal = tf.get_variable(\n",
    "        name='B_al',\n",
    "        shape=[num_AL, 1, dim_sentence],\n",
    "#         initializer=tf.random_uniform_initializer(-0.003, 0.003),\n",
    "        initializer=tf.zeros_initializer(),\n",
    "        regularizer=tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )\n",
    "            \n",
    "with tf.variable_scope('gru', reuse = tf.AUTO_REUSE):\n",
    "    Wr = tf.get_variable(\n",
    "        name='W_r',\n",
    "        shape=[dim_lstm, dim_lstm * 2 + 1],\n",
    "#         initializer=tf.random_uniform_initializer(-0.003, 0.003),\n",
    "        initializer=tf.orthogonal_initializer(),\n",
    "        regularizer=tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )\n",
    "    Wz = tf.get_variable(\n",
    "        name='W_z',\n",
    "        shape=[dim_lstm, dim_lstm * 2 + 1],\n",
    "#         initializer=tf.random_uniform_initializer(-0.003, 0.003),\n",
    "        initializer=tf.orthogonal_initializer(),\n",
    "        regularizer=tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )\n",
    "    Wg = tf.get_variable(\n",
    "        name='W_g',\n",
    "        shape=[dim_lstm, dim_lstm],\n",
    "#         initializer=tf.random_uniform_initializer(-0.003, 0.003),\n",
    "        initializer=tf.orthogonal_initializer(),\n",
    "        regularizer=tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )\n",
    "    Wx = tf.get_variable(\n",
    "        name='W_x',\n",
    "        shape=[dim_lstm, dim_lstm * 2 + 1],\n",
    "#         initializer=tf.random_uniform_initializer(-0.003, 0.003),\n",
    "        initializer=tf.orthogonal_initializer(),\n",
    "        regularizer=tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )\n",
    "    Ur = tf.get_variable(\n",
    "        name='U_r',\n",
    "        shape=[dim_lstm, dim_lstm],\n",
    "#         initializer=tf.random_uniform_initializer(-0.003, 0.003),\n",
    "        initializer=tf.orthogonal_initializer(),\n",
    "        regularizer=tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )\n",
    "    Uz = tf.get_variable(\n",
    "        name='U_z',\n",
    "        shape=[dim_lstm, dim_lstm],\n",
    "#         initializer=tf.random_uniform_initializer(-0.003, 0.003),\n",
    "        initializer=tf.orthogonal_initializer(),\n",
    "        regularizer=tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )\n",
    "    \n",
    "with tf.variable_scope('softmax', reuse = tf.AUTO_REUSE):\n",
    "    Ws = tf.get_variable(\n",
    "        name='W_s',\n",
    "        shape=[dim_lstm, dim_polarity],\n",
    "#         initializer=tf.random_uniform_initializer(-0.003, 0.003),\n",
    "        initializer=tf.contrib.layers.xavier_initializer(),\n",
    "        regularizer=tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )\n",
    "    Bs = tf.get_variable(\n",
    "        name='B_s',\n",
    "        shape=[dim_polarity],\n",
    "#         initializer=tf.random_uniform_initializer(-0.003, 0.003),\n",
    "        initializer=tf.zeros_initializer(),\n",
    "        regularizer=tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_lstm(inputs, seqlen, weights, aspects):\n",
    "    inputs = tf.cast(tf.nn.dropout(inputs, keep_prob=0.5), tf.float32)\n",
    "    with tf.name_scope('bilstm_model'):\n",
    "        forward_lstm_cell = tf.contrib.rnn.LSTMCell(dim_lstm) \n",
    "        backward_lstm_cell = tf.contrib.rnn.LSTMCell(dim_lstm)\n",
    "        Mstar, states = tf.nn.bidirectional_dynamic_rnn(\n",
    "            forward_lstm_cell,\n",
    "            backward_lstm_cell,\n",
    "#             tf.unstack(tf.transpose(inputs, perm=[1, 0])),\n",
    "            inputs = inputs,\n",
    "            sequence_length = seqlen,\n",
    "            dtype = tf.float32,\n",
    "            scope = 'bilstm6'\n",
    "        )\n",
    "#         fw, bk = Mstar\n",
    "#         bk = tf.reverse_sequence(bk, tf.cast(seqlen, tf.int64), seq_dim=1)\n",
    "        Mstar = tf.concat(Mstar, 2)\n",
    "        # Mstar is batch_size x [(dim_sentence x dim_lstm), (dim_sentence x dim_lstm)]\n",
    "#         print(tf.concat(Mstar, 1).get_shape())\n",
    "#         Mstar = tf.reshape(tf.concat(Mstar, 1), [-1, dim_sentence, dim_lstm * 2]) # batch_size x dim_sentence x (dim_lstm * 2)\n",
    "#         print(Mstar.get_shape())\n",
    "        batch_size = tf.shape(Mstar)[0]\n",
    "        u = 1 - weights # batch_size x dim_sentence\n",
    "        weights = [weights for i in range(dim_lstm * 2)] # stack weights vectors\n",
    "        weights = tf.stack(weights, 1) # batch_size x (dim_lstm * 2) x dim_sentence\n",
    "        weights = tf.reshape(weights, [-1, dim_sentence, dim_lstm * 2])\n",
    "        M = tf.multiply(Mstar, weights) # batch_size x dim_sentence x (dim_lstm * 2)\n",
    "        u = tf.reshape(u, [-1, dim_sentence])\n",
    "        M = tf.reshape(M, [-1, dim_sentence * (dim_lstm * 2)])\n",
    "        M = tf.concat([M, u], 1) # batch_size x (dim_sentence * (dim_lstm * 2 + 1))\n",
    "        M = tf.reshape(M, [-1, dim_sentence, dim_lstm * 2 + 1]) # batch_size x dim_sentence x (dim_lstm * 2 + 1)\n",
    "        \n",
    "        # recurrently update attention on memory (M)\n",
    "        e = tf.zeros([batch_size, dim_lstm])\n",
    "#         i_AL = tf.random_uniform([batch_size, dim_lstm * 2 + 1], -0.5, 0.5)\n",
    "#         i_AL = tf.ones([batch_size, dim_lstm * 2 + 1])\n",
    "#         e_stack = [e for i in range(dim_sentence)]\n",
    "#         e_stack = tf.stack(e_stack, 1)\n",
    "#         e_stack = tf.reshape(e_stack, [-1, dim_lstm])\n",
    "#         aspects_stack = [aspects for i in range(dim_sentence)]\n",
    "#         aspects_stack = tf.stack(aspects_stack, 1)\n",
    "#         aspects_stack = tf.reshape(aspects_stack, [-1, dim_word])\n",
    "#         e_aspects_concat = tf.concat([e_stack, aspects_stack], 1)\n",
    "#         M_unfold = tf.reshape(M, [-1, dim_lstm * 2 + 1])\n",
    "#         all_concat = tf.concat([M_unfold, e_aspects_concat], 1) # (batch_size * dim_sentence) x (dim_lstm * 2 + 1 + dim_lstm + dim_word)\n",
    "# #         all_concat = tf.reshape(all_concat, [batch_size, dim_sentence, -1])\n",
    "#         g = tf.matmul(Wal[0], tf.transpose(all_concat)) # 1 x (batch_size * dim_sentence)\n",
    "#         alpha = tf.nn.softmax(tf.reshape(g, [-1]))\n",
    "#         alpha = tf.reshape(alpha, [-1, 1, dim_sentence]) # batch_size x 1 x dim_sentence\n",
    "#         i_AL = tf.matmul(alpha, M) # batch_size x 1 x (dim_lstm * 2 + 1)\n",
    "#         i_AL = tf.reshape(i_AL, [-1, dim_lstm * 2 + 1]) # batch_size x (dim_lstm * 2 + 1)\n",
    "        for al in range(num_AL):\n",
    "            \n",
    "            \n",
    "            # update i_AL\n",
    "            e_stack = [e for i in range(dim_sentence)]\n",
    "            e_stack = tf.stack(e_stack, 1)\n",
    "            e_stack = tf.reshape(e_stack, [-1, dim_lstm])\n",
    "            aspects_stack = [aspects for i in range(dim_sentence)]\n",
    "            aspects_stack = tf.stack(aspects_stack, 1)\n",
    "            aspects_stack = tf.reshape(aspects_stack, [-1, dim_word])\n",
    "            e_aspects_concat = tf.concat([e_stack, aspects_stack], 1)\n",
    "            M_unfold = tf.reshape(M, [-1, dim_lstm * 2 + 1])\n",
    "            all_concat = tf.concat([M_unfold, e_aspects_concat], 1) # (batch_size * dim_sentence) x (dim_lstm * 2 + 1 + dim_lstm + dim_word)\n",
    "            g = tf.matmul(Wal[al], tf.transpose(all_concat)) # 1 x (batch_size * dim_sentence)\n",
    "            alpha = tf.nn.softmax(tf.reshape(g, [-1]))\n",
    "            alpha = tf.reshape(alpha, [-1, 1, dim_sentence]) # batch_size x 1 x dim_sentence\n",
    "            i_AL = tf.matmul(alpha, M) # batch_size x 1 x (dim_lstm * 2 + 1)\n",
    "            i_AL = tf.reshape(i_AL, [-1, dim_lstm * 2 + 1]) # batch_size x (dim_lstm * 2 + 1)\n",
    "#             print(i_AL)\n",
    "            # last step in gru: update e\n",
    "            # gru\n",
    "            r = tf.nn.sigmoid(tf.matmul(i_AL, tf.transpose(Wr)) + tf.matmul(e, Ur)) # batch_size x dim_lstm\n",
    "            z = tf.nn.sigmoid(tf.matmul(i_AL, tf.transpose(Wz)) + tf.matmul(e, Uz)) # batch_size x dim_lstm\n",
    "            e_temp = tf.nn.tanh(tf.matmul(i_AL, tf.transpose(Wx)) + tf.matmul(tf.multiply(r, e), Wg)) # batch_size x dim_lstm\n",
    "            e = tf.multiply((1 - z), e) + tf.multiply(z, e_temp) # batch_size x dim_lstm\n",
    "            \n",
    "        predict = tf.matmul(e, Ws) + Bs # dim_polarity\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary_loss = tf.summary.scalar('loss', cost)\n",
    "# summary_acc = tf.summary.scalar('acc', accuracy)\n",
    "# train_summary_op = tf.summary.merge([summary_loss, summary_acc])\n",
    "# test_summary_op = tf.summary.merge([summary_loss, summary_acc])\n",
    "# _dir = 'logs/' + str(timestamp) + '_r' + str(learning_rate) + '_b' + str(batch_size) + '_l' + str(reg_eta)\n",
    "# train_summary_writer = tf.summary.FileWriter(_dir + '/train', sess.graph)\n",
    "# test_summary_writer = tf.summary.FileWriter(_dir + '/test', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 160, 300)\n"
     ]
    }
   ],
   "source": [
    "predict = dynamic_lstm(tf.nn.embedding_lookup(glove, X), seqlen, sentence_locs, aspects)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = predict, labels = y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "correct = tf.equal(tf.argmax(predict, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train loss: 0.9493002219527376, train accuracy: 0.6012867648227542\n",
      "None\n",
      "epoch: 0, valid loss: 1.0244383, valid accuracy: 0.5972222\n",
      "epoch: 1, train loss: 0.9158991522648755, train accuracy: 0.6018382351772458\n",
      "None\n",
      "epoch: 1, valid loss: 1.0038958, valid accuracy: 0.5972222\n",
      "epoch: 2, train loss: 0.9005189280883938, train accuracy: 0.5997549020776561\n",
      "None\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-14ff94b1d96c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch: %s, train loss: %s, train accuracy: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalid_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalid_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseqlen\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalid_seqlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_locs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalid_word_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspects\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalid_aspects_encoding\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch: %s, valid loss: %s, valid accuracy: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#     loss_test, accuracy_test = sess.run([loss, accuracy], feed_dict = {X: test_X, y: test_y, seqlen: test_seqlen, sentence_locs: test_word_weights, aspects: test_aspects_encoding})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(full_iterations):\n",
    "        index = get_batch_index(len(train_X), batch_size, True)\n",
    "        l = 0.\n",
    "        a = 0.\n",
    "        c = 0\n",
    "        for j in range(int(len(index) / batch_size) + (1 if len(index) % batch_size else 0)):\n",
    "            _, batch_loss, batch_accuracy = sess.run([optimizer, loss, accuracy], feed_dict = {X: train_X[index[j * batch_size : (j + 1) * batch_size]], y: train_y[index[j * batch_size : (j + 1) * batch_size]], seqlen: train_seqlen[index[j * batch_size : (j + 1) * batch_size]], sentence_locs: train_word_weights[index[j * batch_size : (j + 1) * batch_size]], aspects: train_aspects_encoding[index[j * batch_size : (j + 1) * batch_size]]})\n",
    "            l += batch_loss\n",
    "            a += batch_accuracy\n",
    "            c += 1\n",
    "        print(print('epoch: %s, train loss: %s, train accuracy: %s' % (i, l/c, a/c)))\n",
    "        loss_valid, accuracy_valid = sess.run([loss, accuracy], feed_dict = {X: valid_X, y: valid_y, seqlen: valid_seqlen, sentence_locs: valid_word_weights, aspects: valid_aspects_encoding})\n",
    "        print('epoch: %s, valid loss: %s, valid accuracy: %s' % (i, loss_valid, accuracy_valid))\n",
    "#     loss_test, accuracy_test = sess.run([loss, accuracy], feed_dict = {X: test_X, y: test_y, seqlen: test_seqlen, sentence_locs: test_word_weights, aspects: test_aspects_encoding})\n",
    "#     print('step: %s, valid loss: %s, valid accuracy: %s' % (i, loss_test, accuracy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = tf.constant([[[1.0, 2.0, 3.0],[4.,5.,6.]],[[7.0, 8.0, 9.0],[10.,11.,12.]]])\n",
    "d = tf.cast(tf.constant(np.ones((2,2,3))), tf.float32)\n",
    "e = tf.multiply(c,d)\n",
    "f = 1 - c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(np.arange(1, 13, dtype=np.int32),\n",
    "                shape=[2, 6])\n",
    "b = tf.constant(np.arange(13, 25, dtype=np.int32),\n",
    "                shape=[6, 2])\n",
    "c = tf.matmul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove[2083]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_y[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_seqlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_aspects_encoding[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_word_weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.random_uniform([4], -0.003, 0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_index(length, batch_size, is_shuffle=True):\n",
    "    index = list(range(length))\n",
    "    if is_shuffle:\n",
    "        np.random.shuffle(index)\n",
    "    return index\n",
    "#     for i in range(int(length / batch_size) + (1 if length % batch_size else 0)):\n",
    "#          index[i * batch_size:(i + 1) * batch_size]\n",
    "        \n",
    "# def get_batch_data(sentences, aspects, sentence_lens, sentence_locs, labels, batch_size, is_shuffle):\n",
    "#         for index in get_batch_index(len(sentences), batch_size, is_shuffle):\n",
    "#             feed_dict = {\n",
    "#                 X: sentences[index],\n",
    "#                 aspects: aspects[index],\n",
    "#                 seqlen: sentence_lens[index],\n",
    "#                 sentence_locs: sentence_locs[index],\n",
    "#                 y: labels[index],\n",
    "#             }\n",
    "#             yield feed_dict, len(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch, num in get_batch_data(train_X, train_aspects_encoding, train_seqlen, train_word_weights, train_y, batch_size, True):\n",
    "    print(num)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_batch_index(1000, 32, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(int(1000 / batch_size) + (1 if 1000 % batch_size else 0)):\n",
    "    print(len(test[i * batch_size : (i + 1) * batch_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0\n",
    "t +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.ones((5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[[3,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_weights[[1,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_seqlen[[1,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array(valid_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[[1,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(np.arange(1,13,dtype=np.int32))\n",
    "b = tf.constant(np.arange(13,25,dtype=np.int32))\n",
    "a = tf.reshape(a, [2,6])\n",
    "b = tf.reshape(b, [2,6])\n",
    "c = tf.concat([a,b],0)\n",
    "c = tf.reshape(c, [2, 12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
       "       [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
