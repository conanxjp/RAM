{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_iterations = 500\n",
    "batch_size = 27\n",
    "full_iterations = 100\n",
    "learning_rate = 0.001\n",
    "reg_eta = 0.001\n",
    "\n",
    "# dimensionalities\n",
    "dim_lstm = 300\n",
    "dim_word = 300\n",
    "dim_aspect = 5\n",
    "dim_aspect_embedding = 300\n",
    "dim_sentence = 80\n",
    "dim_polarity = 3\n",
    "\n",
    "# setup utils object\n",
    "isSample = False\n",
    "u = utils.UTILS(batch_size, dim_sentence, dim_polarity, isSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tf placeholders\n",
    "X = tf.placeholder(tf.int32, [None, dim_sentence])\n",
    "y = tf.placeholder(tf.float32, [None, dim_polarity])\n",
    "seqlen = tf.placeholder(tf.int32, [None])\n",
    "aspects = tf.placeholder(tf.int32, [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "# define tf variables\n",
    "with tf.variable_scope('aspect_embedding_vars', reuse = tf.AUTO_REUSE):\n",
    "    va = tf.get_variable(\n",
    "        name = 'aspect_matrix_Va',\n",
    "        shape = [dim_aspect, dim_aspect_embedding],\n",
    "        initializer = tf.random_normal_initializer(0, 0.003),\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )\n",
    "    wv = tf.get_variable(\n",
    "        name = 'aspect_Wv',\n",
    "        shape = [dim_aspect_embedding, dim_aspect_embedding],\n",
    "        initializer = tf.random_normal_initializer(0, 0.003),\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )\n",
    "with tf.variable_scope('attention_vars', reuse = tf.AUTO_REUSE):\n",
    "    wh = tf.get_variable(\n",
    "        name = 'M_tanh_Wh',\n",
    "        shape = [dim_lstm, dim_lstm],\n",
    "        initializer = tf.random_normal_initializer(0, 0.003),\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )\n",
    "    w = tf.get_variable(\n",
    "        name = 'alpha_softmax_W',\n",
    "        shape = [dim_lstm + dim_aspect_embedding, 1],\n",
    "        initializer = tf.random_normal_initializer(0, 0.003),\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )\n",
    "    wp = tf.get_variable(\n",
    "        name = 'hstar_tanh_Wp',\n",
    "        shape = [dim_lstm, dim_lstm],\n",
    "        initializer = tf.random_normal_initializer(0, 0.003),\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )\n",
    "    wx = tf.get_variable(\n",
    "        name = 'hstar_tanh_Wx',\n",
    "        shape = [dim_lstm, dim_lstm],\n",
    "        initializer = tf.random_normal_initializer(0, 0.003),\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )\n",
    "with tf.variable_scope('output_softmax_vars', reuse = tf.AUTO_REUSE):\n",
    "    ws = tf.get_variable(\n",
    "        name = 'y_softmax_Ws',\n",
    "        shape = [dim_lstm, dim_polarity],\n",
    "        initializer = tf.random_normal_initializer(0, 0.003),\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )\n",
    "    bs = tf.get_variable(\n",
    "        name = 'y_softmax_Bs',\n",
    "        shape = [dim_polarity],\n",
    "        initializer = tf.random_normal_initializer(0, 0.003),\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define lstm model\n",
    "def dynamic_lstm(inputs, seqlen, aspects):\n",
    "    inputs = tf.nn.dropout(inputs, keep_prob=1.0)\n",
    "    with tf.name_scope('lstm_model'):\n",
    "        # slice the corresponding vai from va\n",
    "        vai = tf.gather(va, aspects) # batch_size x dim_aspect_embedding\n",
    "        # concatenate vai to inputs\n",
    "        vai_en = [vai for i in range(dim_sentence)]\n",
    "        vai_en = tf.stack(vai_en, axis = 1) # batch_size x dim_sentence x dim_aspect_embedding\n",
    "        inputs = tf.concat([inputs, vai_en], 2)\n",
    "        lstm_cell = tf.contrib.rnn.LSTMCell(dim_lstm)\n",
    "        H, state = tf.nn.dynamic_rnn(\n",
    "            lstm_cell,\n",
    "            inputs = inputs,\n",
    "            sequence_length = seqlen,\n",
    "            dtype = tf.float32,\n",
    "            scope = 'lstm'\n",
    "        )\n",
    "        size = tf.shape(H)[0]\n",
    "        wv_vai = tf.matmul(vai, wv) # batch_size x dim_aspect_embedding\n",
    "        # stacking Wv x Va along sentence length\n",
    "        wv_vai = [wv_vai for i in range(dim_sentence)]\n",
    "        wv_vai_en = tf.stack(wv_vai, axis = 1) # batch_size x dim_sentence x dim_aspect_embedding\n",
    "        wv_vai_en = tf.reshape(wv_vai_en, [-1, dim_aspect_embedding]) # (batch_size * dim_sentence) x dim_aspect_embedding\n",
    "        H_1 = tf.reshape(H, [-1, dim_lstm]) # (batch_size * dim_sentence) x dim_lstm\n",
    "        wh_H = tf.matmul(H_1, wh) # (batch_size * dim_sentence) x dim_lstm\n",
    "        # concatenate wh_H and wv_va_En for inputting to tanh\n",
    "        wh_H_wv_vai_en = tf.concat([wh_H, wv_vai_en], 1) # (batch_size * dim_sentence) x (dim_lstm + dim_aspect_embedding)\n",
    "        M = tf.tanh(wh_H_wv_vai_en) # (batch_size * dim_sentence) x (dim_lstm + dim_aspect_embedding)\n",
    "        alpha = tf.nn.softmax(tf.matmul(M, w)) # (batch_size * dim_sentence)\n",
    "        alpha = tf.reshape(alpha, [-1, 1, dim_sentence]) # batch_size x 1 x dim_sentence\n",
    "        index = tf.range(0, size) * dim_sentence + seqlen - 1 # batch_size\n",
    "        hn = tf.gather(tf.reshape(H, [-1, dim_lstm]), index)  # batch_size x dim_lstm\n",
    "        r = tf.reshape(tf.matmul(alpha, H), [-1, dim_lstm]) # batch_size x dim_lstm\n",
    "        h_star = tf.tanh(tf.matmul(r, wp) + tf.matmul(hn, wx)) # batch_size x dim_lstm\n",
    "        predict = tf.matmul(h_star, ws) + bs # batch x dim_polarity\n",
    "    return predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# define operations\n",
    "# tf.reset_default_graph()\n",
    "pred = dynamic_lstm(tf.nn.embedding_lookup(u.gloveDict, X), seqlen, aspects)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = pred, labels = y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "correct = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 1.025562, train accuracy: 0.619386\n",
      "step: 0, test loss: 1.0020164, test accuracy: 0.6752312\n",
      "step: 1, train loss: 0.9523352, train accuracy: 0.6196703\n",
      "step: 1, test loss: 0.8878789, test accuracy: 0.6752312\n",
      "step: 2, train loss: 0.93484193, train accuracy: 0.6196703\n",
      "step: 2, test loss: 0.8377855, test accuracy: 0.6752312\n",
      "step: 3, train loss: 0.9338166, train accuracy: 0.6196703\n",
      "step: 3, test loss: 0.82878697, test accuracy: 0.6752312\n",
      "step: 4, train loss: 0.9307274, train accuracy: 0.6196703\n",
      "step: 4, test loss: 0.83139163, test accuracy: 0.6752312\n",
      "step: 5, train loss: 0.9283689, train accuracy: 0.6196703\n",
      "step: 5, test loss: 0.8383686, test accuracy: 0.6752312\n",
      "step: 6, train loss: 0.92000675, train accuracy: 0.6196703\n",
      "step: 6, test loss: 0.8371705, test accuracy: 0.6752312\n",
      "step: 7, train loss: 0.91507787, train accuracy: 0.6196703\n",
      "step: 7, test loss: 0.8363536, test accuracy: 0.6752312\n",
      "step: 8, train loss: 0.9170842, train accuracy: 0.6196703\n",
      "step: 8, test loss: 0.8423439, test accuracy: 0.6752312\n",
      "step: 9, train loss: 0.9178109, train accuracy: 0.6196703\n",
      "step: 9, test loss: 0.8486688, test accuracy: 0.6752312\n",
      "step: 10, train loss: 0.9177036, train accuracy: 0.619386\n",
      "step: 10, test loss: 0.85218096, test accuracy: 0.6752312\n",
      "step: 11, train loss: 0.9151273, train accuracy: 0.619386\n",
      "step: 11, test loss: 0.8485444, test accuracy: 0.6752312\n",
      "step: 12, train loss: 0.910741, train accuracy: 0.619386\n",
      "step: 12, test loss: 0.8395988, test accuracy: 0.6752312\n",
      "step: 13, train loss: 0.906611, train accuracy: 0.619386\n",
      "step: 13, test loss: 0.82878953, test accuracy: 0.6752312\n",
      "step: 14, train loss: 0.9041502, train accuracy: 0.61910176\n",
      "step: 14, test loss: 0.8189268, test accuracy: 0.6752312\n",
      "step: 15, train loss: 0.9008273, train accuracy: 0.61910176\n",
      "step: 15, test loss: 0.8180769, test accuracy: 0.6752312\n",
      "step: 16, train loss: 0.8929195, train accuracy: 0.61910176\n",
      "step: 16, test loss: 0.8083142, test accuracy: 0.6752312\n",
      "step: 17, train loss: 0.88788915, train accuracy: 0.61910176\n",
      "step: 17, test loss: 0.7999821, test accuracy: 0.6752312\n",
      "step: 18, train loss: 0.87293214, train accuracy: 0.61910176\n",
      "step: 18, test loss: 0.7886337, test accuracy: 0.6752312\n",
      "step: 19, train loss: 0.8661838, train accuracy: 0.61910176\n",
      "step: 19, test loss: 0.78553003, test accuracy: 0.6752312\n",
      "step: 20, train loss: 0.85857755, train accuracy: 0.61910176\n",
      "step: 20, test loss: 0.7784256, test accuracy: 0.6752312\n",
      "step: 21, train loss: 0.8499132, train accuracy: 0.61910176\n",
      "step: 21, test loss: 0.77415395, test accuracy: 0.6752312\n",
      "step: 22, train loss: 0.84190965, train accuracy: 0.61910176\n",
      "step: 22, test loss: 0.76712734, test accuracy: 0.6752312\n",
      "step: 23, train loss: 0.8356546, train accuracy: 0.61910176\n",
      "step: 23, test loss: 0.7587297, test accuracy: 0.6752312\n",
      "step: 24, train loss: 0.8205208, train accuracy: 0.61910176\n",
      "step: 24, test loss: 0.74645776, test accuracy: 0.6752312\n",
      "step: 25, train loss: 0.8092725, train accuracy: 0.6324616\n",
      "step: 25, test loss: 0.73628163, test accuracy: 0.69167525\n",
      "step: 26, train loss: 0.80499554, train accuracy: 0.64894825\n",
      "step: 26, test loss: 0.7264014, test accuracy: 0.7163412\n",
      "step: 27, train loss: 0.80352956, train accuracy: 0.65548605\n",
      "step: 27, test loss: 0.73480517, test accuracy: 0.70709145\n",
      "step: 28, train loss: 0.7741724, train accuracy: 0.66998297\n",
      "step: 28, test loss: 0.6954183, test accuracy: 0.7338129\n",
      "step: 29, train loss: 0.77869797, train accuracy: 0.66316086\n",
      "step: 29, test loss: 0.6935014, test accuracy: 0.7307297\n",
      "step: 30, train loss: 0.7830064, train accuracy: 0.6679932\n",
      "step: 30, test loss: 0.71555394, test accuracy: 0.70709145\n",
      "step: 31, train loss: 0.74486065, train accuracy: 0.6873223\n",
      "step: 31, test loss: 0.66938335, test accuracy: 0.73586845\n",
      "step: 32, train loss: 0.7752174, train accuracy: 0.6654349\n",
      "step: 32, test loss: 0.68787366, test accuracy: 0.7348407\n",
      "step: 33, train loss: 0.73886704, train accuracy: 0.6913019\n",
      "step: 33, test loss: 0.6710441, test accuracy: 0.7389517\n",
      "step: 34, train loss: 0.7415717, train accuracy: 0.68902785\n",
      "step: 34, test loss: 0.67626935, test accuracy: 0.7327852\n",
      "step: 35, train loss: 0.7303512, train accuracy: 0.6881751\n",
      "step: 35, test loss: 0.65111446, test accuracy: 0.74614596\n",
      "step: 36, train loss: 0.717889, train accuracy: 0.69556564\n",
      "step: 36, test loss: 0.6412832, test accuracy: 0.7564234\n",
      "step: 37, train loss: 0.71938306, train accuracy: 0.7006822\n",
      "step: 37, test loss: 0.6551357, test accuracy: 0.7410072\n",
      "step: 38, train loss: 0.6982434, train accuracy: 0.7077885\n",
      "step: 38, test loss: 0.6311945, test accuracy: 0.75847894\n",
      "step: 39, train loss: 0.70972365, train accuracy: 0.6975554\n",
      "step: 39, test loss: 0.63211936, test accuracy: 0.7636177\n",
      "step: 40, train loss: 0.6828641, train accuracy: 0.7146106\n",
      "step: 40, test loss: 0.6131072, test accuracy: 0.7708119\n",
      "step: 41, train loss: 0.6975667, train accuracy: 0.7040932\n",
      "step: 41, test loss: 0.63350606, test accuracy: 0.74409044\n",
      "step: 42, train loss: 0.67099196, train accuracy: 0.7157476\n",
      "step: 42, test loss: 0.6029295, test accuracy: 0.7636177\n",
      "step: 43, train loss: 0.6787069, train accuracy: 0.7066515\n",
      "step: 43, test loss: 0.6089318, test accuracy: 0.7677287\n",
      "step: 44, train loss: 0.6586516, train accuracy: 0.7197271\n",
      "step: 44, test loss: 0.59578687, test accuracy: 0.7667009\n",
      "step: 45, train loss: 0.656619, train accuracy: 0.71660036\n",
      "step: 45, test loss: 0.59597784, test accuracy: 0.76156217\n",
      "step: 46, train loss: 0.64228904, train accuracy: 0.72370666\n",
      "step: 46, test loss: 0.5836888, test accuracy: 0.77697843\n",
      "step: 47, train loss: 0.6305578, train accuracy: 0.735361\n",
      "step: 47, test loss: 0.5774602, test accuracy: 0.78006166\n",
      "step: 48, train loss: 0.6282887, train accuracy: 0.73877203\n",
      "step: 48, test loss: 0.58294094, test accuracy: 0.77389514\n",
      "step: 49, train loss: 0.6096727, train accuracy: 0.7487209\n",
      "step: 49, test loss: 0.5626459, test accuracy: 0.78006166\n",
      "step: 50, train loss: 0.6122527, train accuracy: 0.7416146\n",
      "step: 50, test loss: 0.561118, test accuracy: 0.7831449\n",
      "step: 51, train loss: 0.59634125, train accuracy: 0.7552587\n",
      "step: 51, test loss: 0.55260724, test accuracy: 0.77697843\n",
      "step: 52, train loss: 0.5915518, train accuracy: 0.7592382\n",
      "step: 52, test loss: 0.5490073, test accuracy: 0.7749229\n",
      "step: 53, train loss: 0.58854705, train accuracy: 0.75611144\n",
      "step: 53, test loss: 0.5445522, test accuracy: 0.7821172\n",
      "step: 54, train loss: 0.57527477, train accuracy: 0.7649233\n",
      "step: 54, test loss: 0.52957886, test accuracy: 0.7862282\n",
      "step: 55, train loss: 0.5746198, train accuracy: 0.76606023\n",
      "step: 55, test loss: 0.5290429, test accuracy: 0.7852004\n",
      "step: 56, train loss: 0.56526977, train accuracy: 0.7706083\n",
      "step: 56, test loss: 0.5258136, test accuracy: 0.7862282\n",
      "step: 57, train loss: 0.5566384, train accuracy: 0.7725981\n",
      "step: 57, test loss: 0.5226813, test accuracy: 0.7821172\n",
      "step: 58, train loss: 0.5499288, train accuracy: 0.77487206\n",
      "step: 58, test loss: 0.51636374, test accuracy: 0.7852004\n",
      "step: 59, train loss: 0.5475324, train accuracy: 0.78055716\n",
      "step: 59, test loss: 0.5143193, test accuracy: 0.79239464\n",
      "step: 60, train loss: 0.54318357, train accuracy: 0.7791359\n",
      "step: 60, test loss: 0.51063997, test accuracy: 0.78828365\n",
      "step: 61, train loss: 0.53404146, train accuracy: 0.7842524\n",
      "step: 61, test loss: 0.5140878, test accuracy: 0.79445016\n",
      "step: 62, train loss: 0.5256232, train accuracy: 0.78595793\n",
      "step: 62, test loss: 0.50403285, test accuracy: 0.79445016\n",
      "step: 63, train loss: 0.51655275, train accuracy: 0.79363275\n",
      "step: 63, test loss: 0.49913633, test accuracy: 0.8016444\n",
      "step: 64, train loss: 0.50931066, train accuracy: 0.7964753\n",
      "step: 64, test loss: 0.49204385, test accuracy: 0.8047277\n",
      "step: 65, train loss: 0.50522375, train accuracy: 0.7964753\n",
      "step: 65, test loss: 0.5045862, test accuracy: 0.79445016\n",
      "step: 66, train loss: 0.5163159, train accuracy: 0.79164296\n",
      "step: 66, test loss: 0.5068312, test accuracy: 0.7759507\n",
      "step: 67, train loss: 0.5296067, train accuracy: 0.7785674\n",
      "step: 67, test loss: 0.53669417, test accuracy: 0.8016444\n",
      "step: 68, train loss: 0.5480975, train accuracy: 0.7706083\n",
      "step: 68, test loss: 0.5340248, test accuracy: 0.7667009\n",
      "step: 69, train loss: 0.48538023, train accuracy: 0.8089824\n",
      "step: 69, test loss: 0.49871996, test accuracy: 0.7903392\n",
      "step: 70, train loss: 0.54168034, train accuracy: 0.766913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 70, test loss: 0.5597953, test accuracy: 0.7893114\n",
      "step: 71, train loss: 0.47234064, train accuracy: 0.81552017\n",
      "step: 71, test loss: 0.4809374, test accuracy: 0.7995889\n",
      "step: 72, train loss: 0.50796145, train accuracy: 0.789369\n",
      "step: 72, test loss: 0.5051105, test accuracy: 0.7852004\n",
      "step: 73, train loss: 0.45875588, train accuracy: 0.8189312\n",
      "step: 73, test loss: 0.4801913, test accuracy: 0.7975334\n",
      "step: 74, train loss: 0.49424034, train accuracy: 0.79278\n",
      "step: 74, test loss: 0.5298467, test accuracy: 0.79239464\n",
      "step: 75, train loss: 0.44405112, train accuracy: 0.8268903\n",
      "step: 75, test loss: 0.48498225, test accuracy: 0.7975334\n",
      "step: 76, train loss: 0.46511844, train accuracy: 0.8158044\n",
      "step: 76, test loss: 0.5026668, test accuracy: 0.7821172\n",
      "step: 77, train loss: 0.43320313, train accuracy: 0.83115405\n",
      "step: 77, test loss: 0.48699474, test accuracy: 0.8016444\n",
      "step: 78, train loss: 0.44065186, train accuracy: 0.82120526\n",
      "step: 78, test loss: 0.5137626, test accuracy: 0.7995889\n",
      "step: 79, train loss: 0.41973892, train accuracy: 0.83541787\n",
      "step: 79, test loss: 0.4991871, test accuracy: 0.8036999\n",
      "step: 80, train loss: 0.4247731, train accuracy: 0.83001703\n",
      "step: 80, test loss: 0.50983393, test accuracy: 0.7821172\n",
      "step: 81, train loss: 0.4432955, train accuracy: 0.8223422\n",
      "step: 81, test loss: 0.5105352, test accuracy: 0.7995889\n",
      "step: 82, train loss: 0.41565844, train accuracy: 0.8357021\n",
      "step: 82, test loss: 0.53081685, test accuracy: 0.77697843\n",
      "step: 83, train loss: 0.3997312, train accuracy: 0.84536666\n",
      "step: 83, test loss: 0.51743233, test accuracy: 0.7903392\n",
      "step: 84, train loss: 0.39423648, train accuracy: 0.8462194\n",
      "step: 84, test loss: 0.50032806, test accuracy: 0.8006166\n",
      "step: 85, train loss: 0.38548726, train accuracy: 0.85048324\n",
      "step: 85, test loss: 0.5054343, test accuracy: 0.78417265\n",
      "step: 86, train loss: 0.371541, train accuracy: 0.8533257\n",
      "step: 86, test loss: 0.51980656, test accuracy: 0.79445016\n",
      "step: 87, train loss: 0.37201414, train accuracy: 0.8564525\n",
      "step: 87, test loss: 0.5293737, test accuracy: 0.8006166\n",
      "step: 88, train loss: 0.36230844, train accuracy: 0.8629903\n",
      "step: 88, test loss: 0.5281656, test accuracy: 0.7821172\n",
      "step: 89, train loss: 0.35061997, train accuracy: 0.8661171\n",
      "step: 89, test loss: 0.52280825, test accuracy: 0.79239464\n",
      "step: 90, train loss: 0.34403065, train accuracy: 0.86895967\n",
      "step: 90, test loss: 0.5411018, test accuracy: 0.79856116\n",
      "step: 91, train loss: 0.33649683, train accuracy: 0.86981237\n",
      "step: 91, test loss: 0.5574665, test accuracy: 0.7780062\n",
      "step: 92, train loss: 0.3449023, train accuracy: 0.86782265\n",
      "step: 92, test loss: 0.5419068, test accuracy: 0.79856116\n",
      "step: 93, train loss: 0.317539, train accuracy: 0.88146675\n",
      "step: 93, test loss: 0.56181437, test accuracy: 0.77697843\n",
      "step: 94, train loss: 0.31170213, train accuracy: 0.8823195\n",
      "step: 94, test loss: 0.5750956, test accuracy: 0.79136693\n",
      "step: 95, train loss: 0.3086978, train accuracy: 0.88544625\n",
      "step: 95, test loss: 0.55898005, test accuracy: 0.7954779\n",
      "step: 96, train loss: 0.29898542, train accuracy: 0.89454234\n",
      "step: 96, test loss: 0.58187574, test accuracy: 0.77389514\n",
      "step: 97, train loss: 0.2895274, train accuracy: 0.89169985\n",
      "step: 97, test loss: 0.5911056, test accuracy: 0.79445016\n",
      "step: 98, train loss: 0.2782941, train accuracy: 0.9007959\n",
      "step: 98, test loss: 0.60125107, test accuracy: 0.7780062\n",
      "step: 99, train loss: 0.2761185, train accuracy: 0.8979534\n",
      "step: 99, test loss: 0.5930015, test accuracy: 0.7903392\n"
     ]
    }
   ],
   "source": [
    "# full dataset training\n",
    "test_X, test_y, test_seqlen, test_aspects = u.getData('test')\n",
    "train_X, train_y, train_seqlen, train_aspects = u.getData('train')\n",
    "results = pd.DataFrame(columns = ['train_accuracy', 'train_loss', 'test_accuracy', 'test_loss'])\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(full_iterations):\n",
    "        sess.run(optimizer, feed_dict = {X: train_X, y: train_y, seqlen: train_seqlen, aspects: train_aspects})\n",
    "#         if i > 0 and i % 4 == 0:\n",
    "        loss_train, accuracy_train = sess.run([loss, accuracy], feed_dict = {X: train_X, y: train_y, seqlen: train_seqlen, aspects: train_aspects})\n",
    "        print('step: %s, train loss: %s, train accuracy: %s' % (i, loss_train, accuracy_train))\n",
    "        loss_test, accuracy_test = sess.run([loss, accuracy], feed_dict = {X: test_X, y: test_y, seqlen: test_seqlen, aspects: test_aspects})\n",
    "        print('step: %s, test loss: %s, test accuracy: %s' % (i, loss_test, accuracy_test))\n",
    "        results.loc[i] = [accuracy_train, loss_train, accuracy_test, loss_test]\n",
    "    results.to_csv('../saved_model/atae_results_%s_%s.csv' % (dim_lstm, dim_aspect_embedding), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4, train loss: 1.0504946, train accuracy: 0.5185185\n",
      "step: 4, test loss: 1.0203229, test accuracy: 0.6125385\n",
      "step: 8, train loss: 1.1566836, train accuracy: 0.22222222\n",
      "step: 8, test loss: 0.97801024, test accuracy: 0.6238438\n",
      "step: 12, train loss: 0.9896913, train accuracy: 0.5555556\n",
      "step: 12, test loss: 0.9519964, test accuracy: 0.6464543\n",
      "step: 16, train loss: 0.98441535, train accuracy: 0.5185185\n",
      "step: 16, test loss: 0.9008282, test accuracy: 0.64028776\n",
      "step: 20, train loss: 0.72792387, train accuracy: 0.7407407\n",
      "step: 20, test loss: 0.9830225, test accuracy: 0.59198356\n",
      "step: 24, train loss: 0.96919394, train accuracy: 0.5555556\n",
      "step: 24, test loss: 0.92547417, test accuracy: 0.6156218\n",
      "step: 28, train loss: 0.8218549, train accuracy: 0.6296296\n",
      "step: 28, test loss: 0.8296099, test accuracy: 0.6608428\n",
      "step: 32, train loss: 0.7712242, train accuracy: 0.7037037\n",
      "step: 32, test loss: 1.1184506, test accuracy: 0.36279547\n",
      "step: 36, train loss: 0.79875267, train accuracy: 0.6666667\n",
      "step: 36, test loss: 1.034779, test accuracy: 0.44707093\n",
      "step: 40, train loss: 0.78609496, train accuracy: 0.7037037\n",
      "step: 40, test loss: 0.8986091, test accuracy: 0.63926\n",
      "step: 44, train loss: 1.1169964, train accuracy: 0.4074074\n",
      "step: 44, test loss: 0.86888695, test accuracy: 0.6300103\n",
      "step: 48, train loss: 0.7663195, train accuracy: 0.6666667\n",
      "step: 48, test loss: 0.8070387, test accuracy: 0.676259\n",
      "step: 52, train loss: 0.7361421, train accuracy: 0.6666667\n",
      "step: 52, test loss: 0.771444, test accuracy: 0.6865365\n",
      "step: 56, train loss: 0.9487258, train accuracy: 0.5555556\n",
      "step: 56, test loss: 0.83386683, test accuracy: 0.655704\n",
      "step: 60, train loss: 0.69825196, train accuracy: 0.7777778\n",
      "step: 60, test loss: 0.69726574, test accuracy: 0.7266187\n",
      "step: 64, train loss: 0.59717834, train accuracy: 0.8148148\n",
      "step: 64, test loss: 0.8165463, test accuracy: 0.64337105\n",
      "step: 68, train loss: 0.88915735, train accuracy: 0.6666667\n",
      "step: 68, test loss: 0.7811559, test accuracy: 0.67934227\n",
      "step: 72, train loss: 0.72905463, train accuracy: 0.7037037\n",
      "step: 72, test loss: 0.7288638, test accuracy: 0.69167525\n",
      "step: 76, train loss: 0.5829508, train accuracy: 0.8148148\n",
      "step: 76, test loss: 0.5747113, test accuracy: 0.7780062\n",
      "step: 80, train loss: 0.6835015, train accuracy: 0.7407407\n",
      "step: 80, test loss: 0.78974605, test accuracy: 0.68037\n",
      "step: 84, train loss: 0.5899176, train accuracy: 0.7777778\n",
      "step: 84, test loss: 0.59990126, test accuracy: 0.7595067\n",
      "step: 88, train loss: 0.5913367, train accuracy: 0.8888889\n",
      "step: 88, test loss: 0.6834653, test accuracy: 0.72970194\n",
      "step: 92, train loss: 0.6607894, train accuracy: 0.7037037\n",
      "step: 92, test loss: 0.77506304, test accuracy: 0.6659815\n",
      "step: 96, train loss: 0.5812957, train accuracy: 0.7407407\n",
      "step: 96, test loss: 0.736904, test accuracy: 0.68139774\n",
      "step: 100, train loss: 0.6051708, train accuracy: 0.7037037\n",
      "step: 100, test loss: 0.60694265, test accuracy: 0.7430627\n",
      "step: 104, train loss: 0.7976494, train accuracy: 0.7037037\n",
      "step: 104, test loss: 0.8664236, test accuracy: 0.65056527\n",
      "step: 108, train loss: 0.8320146, train accuracy: 0.7037037\n",
      "step: 108, test loss: 0.7102001, test accuracy: 0.6998972\n",
      "step: 112, train loss: 0.90414816, train accuracy: 0.7037037\n",
      "step: 112, test loss: 0.63720787, test accuracy: 0.7533402\n",
      "step: 116, train loss: 0.5963281, train accuracy: 0.7777778\n",
      "step: 116, test loss: 0.6351473, test accuracy: 0.7410072\n",
      "step: 120, train loss: 0.7165837, train accuracy: 0.7037037\n",
      "step: 120, test loss: 0.68545455, test accuracy: 0.71531343\n",
      "step: 124, train loss: 0.7544234, train accuracy: 0.7037037\n",
      "step: 124, test loss: 0.7118344, test accuracy: 0.6865365\n",
      "step: 128, train loss: 0.6663783, train accuracy: 0.7407407\n",
      "step: 128, test loss: 0.61493677, test accuracy: 0.7523124\n",
      "step: 132, train loss: 0.78215224, train accuracy: 0.6666667\n",
      "step: 132, test loss: 0.6219151, test accuracy: 0.74717367\n",
      "step: 136, train loss: 0.9312317, train accuracy: 0.6296296\n",
      "step: 136, test loss: 0.75180244, test accuracy: 0.68037\n",
      "step: 140, train loss: 0.6898959, train accuracy: 0.7407407\n",
      "step: 140, test loss: 0.5720453, test accuracy: 0.77697843\n",
      "step: 144, train loss: 0.8921266, train accuracy: 0.5925926\n",
      "step: 144, test loss: 0.62600803, test accuracy: 0.7492292\n",
      "step: 148, train loss: 0.7278311, train accuracy: 0.6666667\n",
      "step: 148, test loss: 0.82069826, test accuracy: 0.6618705\n",
      "step: 152, train loss: 0.7852496, train accuracy: 0.7037037\n",
      "step: 152, test loss: 0.6054559, test accuracy: 0.74820143\n",
      "step: 156, train loss: 0.53200126, train accuracy: 0.8888889\n",
      "step: 156, test loss: 0.57055265, test accuracy: 0.77697843\n",
      "step: 160, train loss: 0.649213, train accuracy: 0.7407407\n",
      "step: 160, test loss: 0.7341791, test accuracy: 0.7225077\n",
      "step: 164, train loss: 0.42357954, train accuracy: 0.9259259\n",
      "step: 164, test loss: 0.6997772, test accuracy: 0.69578624\n",
      "step: 168, train loss: 0.62268907, train accuracy: 0.8148148\n",
      "step: 168, test loss: 0.6010167, test accuracy: 0.7574512\n",
      "step: 172, train loss: 0.5297348, train accuracy: 0.8148148\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-23e2dbcd2acf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mloss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseqlen\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_seqlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspects\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_aspects\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'step: %s, train loss: %s, train accuracy: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mloss_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseqlen\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_seqlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspects\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_aspects\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'step: %s, test loss: %s, test accuracy: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# batch training\n",
    "test_X, test_y, test_seqlen, test_aspects = u.getData('test')\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(batch_iterations):\n",
    "        batch_X, batch_y, batch_seqlen, batch_aspects = u.nextBatch(batch_size)\n",
    "        sess.run(optimizer, feed_dict = {X: batch_X, y: batch_y, seqlen: batch_seqlen, aspects: batch_aspects})\n",
    "        if i > 0 and i % 4 == 0:\n",
    "            loss_train, accuracy_train = sess.run([loss, accuracy], feed_dict = {X: batch_X, y: batch_y, seqlen: batch_seqlen, aspects: batch_aspects})\n",
    "            print('step: %s, train loss: %s, train accuracy: %s' % (i, loss_train, accuracy_train))\n",
    "            loss_test, accuracy_test = sess.run([loss, accuracy], feed_dict = {X: test_X, y: test_y, seqlen: test_seqlen, aspects: test_aspects})\n",
    "            print('step: %s, test loss: %s, test accuracy: %s' % (i, loss_test, accuracy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
