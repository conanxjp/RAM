{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "iterations = 500\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "reg_eta = 0.001\n",
    "\n",
    "# dimensionalities\n",
    "dim_lstm = 300\n",
    "dim_word = 300\n",
    "dim_aspect = 5\n",
    "dim_aspect_embedding = 300\n",
    "dim_sentence = 80\n",
    "dim_polarity = 3\n",
    "\n",
    "# setup utils object\n",
    "u = utils.UTILS(batch_size, dim_sentence, dim_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tf placeholders\n",
    "X = tf.placeholder(tf.int32, [None, dim_sentence])\n",
    "y = tf.placeholder(tf.float32, [None, dim_polarity])\n",
    "seqlen = tf.placeholder(tf.int32, [None])\n",
    "aspects = tf.placeholder(tf.int32, [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "# define tf variables\n",
    "with tf.variable_scope('aspect_embedding_vars', reuse = tf.AUTO_REUSE):\n",
    "    va = tf.get_variable(\n",
    "        name = 'aspect_matrix_Va',\n",
    "        shape = [dim_aspect, dim_aspect_embedding],\n",
    "        initializer = tf.random_normal_initializer(0, 0.003),\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )\n",
    "    wv = tf.get_variable(\n",
    "        name = 'aspect_Wv',\n",
    "        shape = [dim_aspect_embedding, dim_aspect_embedding],\n",
    "        initializer = tf.random_normal_initializer(0, 0.003),\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )\n",
    "with tf.variable_scope('attention_vars', reuse = tf.AUTO_REUSE):\n",
    "    wh = tf.get_variable(\n",
    "        name = 'M_tanh_Wh',\n",
    "        shape = [dim_lstm, dim_lstm],\n",
    "        initializer = tf.random_normal_initializer(0, 0.003),\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )\n",
    "    w = tf.get_variable(\n",
    "        name = 'alpha_softmax_W',\n",
    "        shape = [dim_lstm + dim_aspect_embedding, 1],\n",
    "        initializer = tf.random_normal_initializer(0, 0.003),\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )\n",
    "    wp = tf.get_variable(\n",
    "        name = 'hstar_tanh_Wp',\n",
    "        shape = [dim_lstm, dim_lstm],\n",
    "        initializer = tf.random_normal_initializer(0, 0.003),\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )\n",
    "    wx = tf.get_variable(\n",
    "        name = 'hstar_tanh_Wx',\n",
    "        shape = [dim_lstm, dim_lstm],\n",
    "        initializer = tf.random_normal_initializer(0, 0.003),\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )\n",
    "with tf.variable_scope('output_softmax_vars', reuse = tf.AUTO_REUSE):\n",
    "    ws = tf.get_variable(\n",
    "        name = 'y_softmax_Ws',\n",
    "        shape = [dim_lstm, dim_polarity],\n",
    "        initializer = tf.random_normal_initializer(0, 0.003),\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )\n",
    "    bs = tf.get_variable(\n",
    "        name = 'y_softmax_Bs',\n",
    "        shape = [dim_polarity],\n",
    "        initializer = tf.random_normal_initializer(0, 0.003),\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define lstm model\n",
    "def dynamic_lstm(inputs, seqlen, aspects):\n",
    "    inputs = tf.nn.dropout(inputs, keep_prob=1.0)\n",
    "    with tf.name_scope('lstm_model'):\n",
    "        # slice the corresponding vai from va\n",
    "        vai = tf.gather(va, aspects) # batch_size x dim_aspect_embedding\n",
    "        lstm_cell = tf.contrib.rnn.LSTMCell(dim_lstm)\n",
    "        H, state = tf.nn.dynamic_rnn(\n",
    "            lstm_cell,\n",
    "            inputs = inputs,\n",
    "            sequence_length = seqlen,\n",
    "            dtype = tf.float32,\n",
    "            scope = 'lstm'\n",
    "        )\n",
    "        size = tf.shape(H)[0]\n",
    "        wv_vai = tf.matmul(vai, wv) # batch_size x dim_aspect_embedding\n",
    "        # stacking Wv x Va along sentence length\n",
    "        wv_vai = [wv_vai for i in range(dim_sentence)]\n",
    "        wv_vai_en = tf.stack(wv_vai, axis = 1) # batch_size x dim_sentence x dim_aspect_embedding\n",
    "        wv_vai_en = tf.reshape(wv_vai_en, [-1, dim_aspect_embedding]) # (batch_size * dim_sentence) x dim_aspect_embedding\n",
    "        H_1 = tf.reshape(H, [-1, dim_lstm]) # (batch_size * dim_sentence) x dim_lstm\n",
    "        wh_H = tf.matmul(H_1, wh) # (batch_size * dim_sentence) x dim_lstm\n",
    "        # concatenate wh_H and wv_va_En for inputting to tanh\n",
    "        wh_H_wv_vai_en = tf.concat([wh_H, wv_vai_en], 1) # (batch_size * dim_sentence) x (dim_lstm + dim_aspect_embedding)\n",
    "        M = tf.tanh(wh_H_wv_vai_en) # (batch_size * dim_sentence) x (dim_lstm + dim_aspect_embedding)\n",
    "        alpha = tf.nn.softmax(tf.matmul(M, w)) # (batch_size * dim_sentence)\n",
    "        alpha = tf.reshape(alpha, [-1, 1, dim_sentence]) # batch_size x 1 x dim_sentence\n",
    "        index = tf.range(0, size) * dim_sentence + seqlen - 1 # batch_size\n",
    "        hn = tf.gather(tf.reshape(H, [-1, dim_lstm]), index)  # batch_size x dim_lstm\n",
    "        r = tf.reshape(tf.matmul(alpha, H), [-1, dim_lstm]) # batch_size x dim_lstm\n",
    "        h_star = tf.tanh(tf.matmul(r, wp) + tf.matmul(hn, wx)) # batch_size x dim_lstm\n",
    "        predict = tf.matmul(h_star, ws) + bs # batch x dim_polarity\n",
    "    return predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# define operations\n",
    "# tf.reset_default_graph()\n",
    "pred = dynamic_lstm(tf.nn.embedding_lookup(u.gloveDict, X), seqlen, aspects)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = pred, labels = y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "correct = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full dataset training\n",
    "test_X, test_y, test_seqlen, test_aspects = u.getData('test')\n",
    "train_X, train_y, train_seqlen, train_aspects = u.getData('train')\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(iterations):\n",
    "        sess.run(optimizer, feed_dict = {X: train_X, y: train_y, seqlen: train_seqlen, aspects: train_aspects})\n",
    "#         if i > 0 and i % 4 == 0:\n",
    "        loss_train, accuracy_train = sess.run([loss, accuracy], feed_dict = {X: train_X, y: train_y, seqlen: train_seqlen, aspects: train_aspects})\n",
    "        print('step: %s, train loss: %s, train accuracy: %s' % (i, loss_train, accuracy_train))\n",
    "        loss_test, accuracy_test = sess.run([loss, accuracy], feed_dict = {X: test_X, y: test_y, seqlen: test_seqlen, aspects: test_aspects})\n",
    "        print('step: %s, test loss: %s, test accuracy: %s' % (i, loss_test, accuracy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4, train loss: 1.0564857, train accuracy: 0.4375\n",
      "step: 4, test loss: 1.0706103, test accuracy: 0.3134635\n",
      "step: 8, train loss: 0.94235384, train accuracy: 0.5\n",
      "step: 8, test loss: 1.0103799, test accuracy: 0.5529291\n",
      "step: 12, train loss: 0.79138494, train accuracy: 0.625\n",
      "step: 12, test loss: 0.9852434, test accuracy: 0.52312434\n",
      "step: 16, train loss: 0.8214115, train accuracy: 0.59375\n",
      "step: 16, test loss: 0.7984635, test accuracy: 0.672148\n",
      "step: 20, train loss: 0.9011877, train accuracy: 0.59375\n",
      "step: 20, test loss: 0.8904527, test accuracy: 0.6053443\n",
      "step: 24, train loss: 0.8405943, train accuracy: 0.59375\n",
      "step: 24, test loss: 0.83706146, test accuracy: 0.64028776\n",
      "step: 28, train loss: 0.83595157, train accuracy: 0.59375\n",
      "step: 28, test loss: 1.071208, test accuracy: 0.5508736\n",
      "step: 32, train loss: 0.70952654, train accuracy: 0.71875\n",
      "step: 32, test loss: 1.038372, test accuracy: 0.5601233\n",
      "step: 36, train loss: 0.8234942, train accuracy: 0.625\n",
      "step: 36, test loss: 0.77566665, test accuracy: 0.6865365\n",
      "step: 40, train loss: 0.83643854, train accuracy: 0.625\n",
      "step: 40, test loss: 0.8182173, test accuracy: 0.68037\n",
      "step: 44, train loss: 0.6617017, train accuracy: 0.6875\n",
      "step: 44, test loss: 0.924733, test accuracy: 0.60020554\n",
      "step: 48, train loss: 0.96797186, train accuracy: 0.59375\n",
      "step: 48, test loss: 0.7777553, test accuracy: 0.6855087\n",
      "step: 52, train loss: 0.76594615, train accuracy: 0.6875\n",
      "step: 52, test loss: 0.77271, test accuracy: 0.68756425\n",
      "step: 56, train loss: 0.6988777, train accuracy: 0.71875\n",
      "step: 56, test loss: 0.93709666, test accuracy: 0.63309354\n",
      "step: 60, train loss: 0.8761538, train accuracy: 0.65625\n",
      "step: 60, test loss: 0.67426544, test accuracy: 0.7163412\n",
      "step: 64, train loss: 0.8249955, train accuracy: 0.65625\n",
      "step: 64, test loss: 0.80634254, test accuracy: 0.663926\n",
      "step: 68, train loss: 0.6541733, train accuracy: 0.6875\n",
      "step: 68, test loss: 0.7879643, test accuracy: 0.6752312\n",
      "step: 72, train loss: 0.58920854, train accuracy: 0.78125\n",
      "step: 72, test loss: 0.69366115, test accuracy: 0.7235355\n",
      "step: 76, train loss: 0.68795455, train accuracy: 0.78125\n",
      "step: 76, test loss: 0.7477145, test accuracy: 0.7225077\n",
      "step: 80, train loss: 0.5301683, train accuracy: 0.78125\n",
      "step: 80, test loss: 0.74819124, test accuracy: 0.72764647\n",
      "step: 84, train loss: 0.6793493, train accuracy: 0.6875\n",
      "step: 84, test loss: 0.66490906, test accuracy: 0.7410072\n",
      "step: 88, train loss: 0.7679961, train accuracy: 0.625\n",
      "step: 88, test loss: 0.6697744, test accuracy: 0.72867423\n",
      "step: 92, train loss: 0.6931238, train accuracy: 0.6875\n",
      "step: 92, test loss: 0.7261302, test accuracy: 0.7081192\n",
      "step: 96, train loss: 0.7217691, train accuracy: 0.6875\n",
      "step: 96, test loss: 0.67538726, test accuracy: 0.7266187\n",
      "step: 100, train loss: 0.9015791, train accuracy: 0.6875\n",
      "step: 100, test loss: 0.64469475, test accuracy: 0.7379239\n",
      "step: 104, train loss: 0.6564042, train accuracy: 0.75\n",
      "step: 104, test loss: 0.7404667, test accuracy: 0.700925\n",
      "step: 108, train loss: 0.746359, train accuracy: 0.6875\n",
      "step: 108, test loss: 0.6796606, test accuracy: 0.7307297\n",
      "step: 112, train loss: 0.60425776, train accuracy: 0.84375\n",
      "step: 112, test loss: 0.62918526, test accuracy: 0.73175746\n",
      "step: 116, train loss: 0.73243076, train accuracy: 0.71875\n",
      "step: 116, test loss: 0.6314656, test accuracy: 0.7348407\n",
      "step: 120, train loss: 0.7185166, train accuracy: 0.75\n",
      "step: 120, test loss: 0.6854315, test accuracy: 0.7122302\n",
      "step: 124, train loss: 0.34475368, train accuracy: 0.9375\n",
      "step: 124, test loss: 0.65974826, test accuracy: 0.7348407\n",
      "step: 128, train loss: 0.9029537, train accuracy: 0.625\n",
      "step: 128, test loss: 0.6473728, test accuracy: 0.73586845\n",
      "step: 132, train loss: 0.6416892, train accuracy: 0.78125\n",
      "step: 132, test loss: 0.8039505, test accuracy: 0.66495377\n",
      "step: 136, train loss: 0.7222284, train accuracy: 0.6875\n",
      "step: 136, test loss: 0.64598745, test accuracy: 0.7368962\n",
      "step: 140, train loss: 0.38098282, train accuracy: 0.875\n",
      "step: 140, test loss: 0.5933622, test accuracy: 0.7523124\n",
      "step: 144, train loss: 0.7195052, train accuracy: 0.78125\n",
      "step: 144, test loss: 0.69338286, test accuracy: 0.70298046\n",
      "step: 148, train loss: 0.59535265, train accuracy: 0.75\n",
      "step: 148, test loss: 0.6329946, test accuracy: 0.7368962\n",
      "step: 152, train loss: 0.74481326, train accuracy: 0.75\n",
      "step: 152, test loss: 0.60491663, test accuracy: 0.75436795\n",
      "step: 156, train loss: 0.5303546, train accuracy: 0.84375\n",
      "step: 156, test loss: 0.6901818, test accuracy: 0.71017474\n",
      "step: 160, train loss: 1.1400473, train accuracy: 0.5625\n",
      "step: 160, test loss: 0.611595, test accuracy: 0.742035\n",
      "step: 164, train loss: 0.4388808, train accuracy: 0.84375\n",
      "step: 164, test loss: 0.64895517, test accuracy: 0.7307297\n",
      "step: 168, train loss: 0.39155647, train accuracy: 0.875\n",
      "step: 168, test loss: 0.6558882, test accuracy: 0.7327852\n",
      "step: 172, train loss: 0.46691805, train accuracy: 0.875\n",
      "step: 172, test loss: 0.6489147, test accuracy: 0.73586845\n",
      "step: 176, train loss: 0.53832924, train accuracy: 0.8125\n",
      "step: 176, test loss: 0.70699626, test accuracy: 0.7163412\n",
      "step: 180, train loss: 0.7098813, train accuracy: 0.6875\n",
      "step: 180, test loss: 0.6784819, test accuracy: 0.7204522\n",
      "step: 184, train loss: 0.42550796, train accuracy: 0.875\n",
      "step: 184, test loss: 0.6070639, test accuracy: 0.7410072\n",
      "step: 188, train loss: 0.7432671, train accuracy: 0.65625\n",
      "step: 188, test loss: 0.7470679, test accuracy: 0.6865365\n",
      "step: 192, train loss: 0.6150801, train accuracy: 0.75\n",
      "step: 192, test loss: 0.7123252, test accuracy: 0.69578624\n",
      "step: 196, train loss: 0.5385455, train accuracy: 0.8125\n",
      "step: 196, test loss: 0.6075001, test accuracy: 0.7646454\n",
      "step: 200, train loss: 0.8508749, train accuracy: 0.59375\n",
      "step: 200, test loss: 0.62615275, test accuracy: 0.7523124\n",
      "step: 204, train loss: 0.5077021, train accuracy: 0.8125\n",
      "step: 204, test loss: 0.6987732, test accuracy: 0.7163412\n",
      "step: 208, train loss: 0.5719747, train accuracy: 0.8125\n",
      "step: 208, test loss: 0.6851981, test accuracy: 0.71736896\n",
      "step: 212, train loss: 0.6112058, train accuracy: 0.75\n",
      "step: 212, test loss: 0.57082087, test accuracy: 0.76567316\n",
      "step: 216, train loss: 0.6861584, train accuracy: 0.78125\n",
      "step: 216, test loss: 0.74242455, test accuracy: 0.68448097\n",
      "step: 220, train loss: 0.37844434, train accuracy: 0.875\n",
      "step: 220, test loss: 0.8207237, test accuracy: 0.6865365\n",
      "step: 224, train loss: 0.51503396, train accuracy: 0.71875\n",
      "step: 224, test loss: 0.54185355, test accuracy: 0.7780062\n",
      "step: 228, train loss: 0.6870811, train accuracy: 0.6875\n",
      "step: 228, test loss: 0.7177284, test accuracy: 0.705036\n",
      "step: 232, train loss: 0.7269245, train accuracy: 0.75\n",
      "step: 232, test loss: 0.69504076, test accuracy: 0.7266187\n",
      "step: 236, train loss: 0.5631305, train accuracy: 0.75\n",
      "step: 236, test loss: 0.5529649, test accuracy: 0.7718397\n",
      "step: 240, train loss: 0.5049561, train accuracy: 0.84375\n",
      "step: 240, test loss: 0.61427486, test accuracy: 0.74717367\n",
      "step: 244, train loss: 0.74593097, train accuracy: 0.71875\n",
      "step: 244, test loss: 0.74929523, test accuracy: 0.6824255\n",
      "step: 248, train loss: 0.52473116, train accuracy: 0.75\n",
      "step: 248, test loss: 0.6270415, test accuracy: 0.73997945\n",
      "step: 252, train loss: 0.5385441, train accuracy: 0.78125\n",
      "step: 252, test loss: 0.60991514, test accuracy: 0.74409044\n",
      "step: 256, train loss: 0.5402939, train accuracy: 0.71875\n",
      "step: 256, test loss: 0.6501995, test accuracy: 0.7225077\n",
      "step: 260, train loss: 0.5247879, train accuracy: 0.71875\n",
      "step: 260, test loss: 0.65709364, test accuracy: 0.7451182\n",
      "step: 264, train loss: 0.66910017, train accuracy: 0.75\n",
      "step: 264, test loss: 0.5454962, test accuracy: 0.7893114\n",
      "step: 268, train loss: 0.21414573, train accuracy: 0.96875\n",
      "step: 268, test loss: 0.5739508, test accuracy: 0.7749229\n",
      "step: 272, train loss: 0.40857735, train accuracy: 0.8125\n",
      "step: 272, test loss: 0.60487115, test accuracy: 0.76875645\n",
      "step: 276, train loss: 0.5696373, train accuracy: 0.75\n",
      "step: 276, test loss: 0.64165145, test accuracy: 0.7523124\n",
      "step: 280, train loss: 0.33942905, train accuracy: 0.875\n",
      "step: 280, test loss: 0.7370986, test accuracy: 0.6998972\n",
      "step: 284, train loss: 0.64378524, train accuracy: 0.75\n",
      "step: 284, test loss: 0.62893325, test accuracy: 0.742035\n",
      "step: 288, train loss: 0.5114106, train accuracy: 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 288, test loss: 0.5767456, test accuracy: 0.7677287\n",
      "step: 292, train loss: 0.39509395, train accuracy: 0.875\n",
      "step: 292, test loss: 0.6514637, test accuracy: 0.7379239\n",
      "step: 296, train loss: 0.65705633, train accuracy: 0.8125\n",
      "step: 296, test loss: 0.58328205, test accuracy: 0.7780062\n",
      "step: 300, train loss: 0.32199064, train accuracy: 0.90625\n",
      "step: 300, test loss: 0.60867393, test accuracy: 0.7677287\n",
      "step: 304, train loss: 0.5829632, train accuracy: 0.75\n",
      "step: 304, test loss: 0.8525316, test accuracy: 0.62898254\n",
      "step: 308, train loss: 0.39177847, train accuracy: 0.90625\n",
      "step: 308, test loss: 0.61435246, test accuracy: 0.7749229\n",
      "step: 312, train loss: 0.6058456, train accuracy: 0.625\n",
      "step: 312, test loss: 0.7019048, test accuracy: 0.7235355\n",
      "step: 316, train loss: 0.61384773, train accuracy: 0.625\n",
      "step: 316, test loss: 0.5856152, test accuracy: 0.76156217\n",
      "step: 320, train loss: 0.42325252, train accuracy: 0.875\n",
      "step: 320, test loss: 0.68316525, test accuracy: 0.7204522\n",
      "step: 324, train loss: 0.36011556, train accuracy: 0.84375\n",
      "step: 324, test loss: 0.70674765, test accuracy: 0.7266187\n",
      "step: 328, train loss: 0.3567397, train accuracy: 0.84375\n",
      "step: 328, test loss: 0.6643645, test accuracy: 0.74614596\n",
      "step: 332, train loss: 0.23515876, train accuracy: 0.9375\n",
      "step: 332, test loss: 0.73248297, test accuracy: 0.7338129\n",
      "step: 336, train loss: 0.3794161, train accuracy: 0.90625\n",
      "step: 336, test loss: 0.7481033, test accuracy: 0.7194245\n",
      "step: 340, train loss: 0.5400048, train accuracy: 0.78125\n",
      "step: 340, test loss: 0.62103885, test accuracy: 0.75847894\n",
      "step: 344, train loss: 0.48945197, train accuracy: 0.90625\n",
      "step: 344, test loss: 0.6346561, test accuracy: 0.75847894\n",
      "step: 348, train loss: 0.43719226, train accuracy: 0.84375\n",
      "step: 348, test loss: 0.77234566, test accuracy: 0.71325797\n",
      "step: 352, train loss: 0.41344655, train accuracy: 0.875\n",
      "step: 352, test loss: 0.6029592, test accuracy: 0.7780062\n",
      "step: 356, train loss: 0.39614677, train accuracy: 0.84375\n",
      "step: 356, test loss: 0.6441214, test accuracy: 0.7708119\n",
      "step: 360, train loss: 0.5893973, train accuracy: 0.78125\n",
      "step: 360, test loss: 0.7145757, test accuracy: 0.75847894\n",
      "step: 364, train loss: 0.20525981, train accuracy: 0.9375\n",
      "step: 364, test loss: 0.61129135, test accuracy: 0.7749229\n",
      "step: 368, train loss: 0.37486196, train accuracy: 0.8125\n",
      "step: 368, test loss: 0.6947562, test accuracy: 0.7338129\n",
      "step: 372, train loss: 0.42141426, train accuracy: 0.8125\n",
      "step: 372, test loss: 0.7261117, test accuracy: 0.7040082\n",
      "step: 376, train loss: 0.41185433, train accuracy: 0.875\n",
      "step: 376, test loss: 0.61959773, test accuracy: 0.7379239\n",
      "step: 380, train loss: 0.42416203, train accuracy: 0.875\n",
      "step: 380, test loss: 0.5633615, test accuracy: 0.79136693\n",
      "step: 384, train loss: 0.29682976, train accuracy: 0.84375\n",
      "step: 384, test loss: 0.6175855, test accuracy: 0.7605344\n",
      "step: 388, train loss: 0.18952629, train accuracy: 0.90625\n",
      "step: 388, test loss: 0.59409803, test accuracy: 0.7831449\n",
      "step: 392, train loss: 0.35207185, train accuracy: 0.90625\n",
      "step: 392, test loss: 0.7872995, test accuracy: 0.7307297\n",
      "step: 396, train loss: 0.4943027, train accuracy: 0.8125\n",
      "step: 396, test loss: 0.6836216, test accuracy: 0.7605344\n",
      "step: 400, train loss: 0.4151854, train accuracy: 0.84375\n",
      "step: 400, test loss: 0.61872154, test accuracy: 0.7780062\n",
      "step: 404, train loss: 0.29901922, train accuracy: 0.9375\n",
      "step: 404, test loss: 0.661785, test accuracy: 0.72970194\n",
      "step: 408, train loss: 0.27181777, train accuracy: 0.90625\n",
      "step: 408, test loss: 0.633367, test accuracy: 0.7307297\n",
      "step: 412, train loss: 0.5703269, train accuracy: 0.78125\n",
      "step: 412, test loss: 0.5499763, test accuracy: 0.7759507\n",
      "step: 416, train loss: 0.5697439, train accuracy: 0.78125\n",
      "step: 416, test loss: 0.54371804, test accuracy: 0.78417265\n",
      "step: 420, train loss: 0.36479408, train accuracy: 0.875\n",
      "step: 420, test loss: 0.58405113, test accuracy: 0.7677287\n",
      "step: 424, train loss: 0.49118304, train accuracy: 0.875\n",
      "step: 424, test loss: 0.55220324, test accuracy: 0.79136693\n",
      "step: 428, train loss: 0.39012012, train accuracy: 0.875\n",
      "step: 428, test loss: 0.5619823, test accuracy: 0.7934224\n",
      "step: 432, train loss: 0.3301945, train accuracy: 0.875\n",
      "step: 432, test loss: 0.6675496, test accuracy: 0.7574512\n",
      "step: 436, train loss: 0.22627491, train accuracy: 0.9375\n",
      "step: 436, test loss: 0.5984119, test accuracy: 0.76258993\n",
      "step: 440, train loss: 0.27221125, train accuracy: 0.90625\n",
      "step: 440, test loss: 0.5472504, test accuracy: 0.78828365\n",
      "step: 444, train loss: 0.27286783, train accuracy: 0.90625\n",
      "step: 444, test loss: 0.6111243, test accuracy: 0.77286744\n",
      "step: 448, train loss: 0.17410585, train accuracy: 0.9375\n",
      "step: 448, test loss: 0.60203815, test accuracy: 0.76156217\n",
      "step: 452, train loss: 0.4608627, train accuracy: 0.78125\n",
      "step: 452, test loss: 0.5803156, test accuracy: 0.7759507\n",
      "step: 456, train loss: 0.5698476, train accuracy: 0.84375\n",
      "step: 456, test loss: 0.64739656, test accuracy: 0.75847894\n",
      "step: 460, train loss: 0.50276124, train accuracy: 0.8125\n",
      "step: 460, test loss: 0.6203268, test accuracy: 0.7533402\n",
      "step: 464, train loss: 0.3145179, train accuracy: 0.90625\n",
      "step: 464, test loss: 0.5961698, test accuracy: 0.7708119\n",
      "step: 468, train loss: 0.45650998, train accuracy: 0.84375\n",
      "step: 468, test loss: 0.69323164, test accuracy: 0.7595067\n",
      "step: 472, train loss: 0.31878144, train accuracy: 0.875\n",
      "step: 472, test loss: 0.6356118, test accuracy: 0.7574512\n",
      "step: 476, train loss: 0.5283366, train accuracy: 0.8125\n",
      "step: 476, test loss: 0.6212515, test accuracy: 0.7605344\n",
      "step: 480, train loss: 0.5161081, train accuracy: 0.8125\n",
      "step: 480, test loss: 0.6695194, test accuracy: 0.74614596\n",
      "step: 484, train loss: 0.4010235, train accuracy: 0.84375\n",
      "step: 484, test loss: 0.65342766, test accuracy: 0.7636177\n",
      "step: 488, train loss: 0.4345443, train accuracy: 0.875\n",
      "step: 488, test loss: 0.6012441, test accuracy: 0.7708119\n",
      "step: 492, train loss: 0.24137264, train accuracy: 0.9375\n",
      "step: 492, test loss: 0.6107875, test accuracy: 0.7667009\n",
      "step: 496, train loss: 0.3193181, train accuracy: 0.8125\n",
      "step: 496, test loss: 0.61722857, test accuracy: 0.76978415\n"
     ]
    }
   ],
   "source": [
    "# batch training\n",
    "test_X, test_y, test_seqlen, test_aspects = u.getData('test')\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(iterations):\n",
    "        batch_X, batch_y, batch_seqlen, batch_aspects = u.nextBatch(batch_size)\n",
    "        sess.run(optimizer, feed_dict = {X: batch_X, y: batch_y, seqlen: batch_seqlen, aspects: batch_aspects})\n",
    "        if i > 0 and i % 4 == 0:\n",
    "            loss_train, accuracy_train = sess.run([loss, accuracy], feed_dict = {X: batch_X, y: batch_y, seqlen: batch_seqlen, aspects: batch_aspects})\n",
    "            print('step: %s, train loss: %s, train accuracy: %s' % (i, loss_train, accuracy_train))\n",
    "            loss_test, accuracy_test = sess.run([loss, accuracy], feed_dict = {X: test_X, y: test_y, seqlen: test_seqlen, aspects: test_aspects})\n",
    "            print('step: %s, test loss: %s, test accuracy: %s' % (i, loss_test, accuracy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
