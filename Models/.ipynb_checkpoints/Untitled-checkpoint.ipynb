{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils\n",
    "import random\n",
    "import sys\n",
    "import argparse\n",
    "from nltk import word_tokenize\n",
    "import config as cf\n",
    "\n",
    "# hyperparameters\n",
    "batch_iterations = 11000\n",
    "batch_size = 32\n",
    "full_iterations = 100\n",
    "learning_rate = 0.01\n",
    "reg_eta = 0.001\n",
    "\n",
    "# dimensionalities\n",
    "dim_lstm = 300\n",
    "dim_word = 300\n",
    "dim_aspect = 5\n",
    "dim_aspect_embedding = 300\n",
    "dim_sentence = 80\n",
    "dim_polarity = 3\n",
    "\n",
    "# setup utils object\n",
    "isSample = False\n",
    "u = utils.UTILS(batch_size, dim_sentence, dim_polarity, isSample)\n",
    "\n",
    "# define tf placeholders\n",
    "X = tf.placeholder(tf.int32, [None, dim_sentence])\n",
    "y = tf.placeholder(tf.float32, [None, dim_polarity])\n",
    "seqlen = tf.placeholder(tf.int32, [None])\n",
    "aspects = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "# define tf variables\n",
    "with tf.variable_scope('aspect_embedding_vars', reuse = tf.AUTO_REUSE):\n",
    "    fw_va = tf.get_variable(\n",
    "        name = 'aspect_matrix_forward_Va',\n",
    "        shape = [dim_aspect, dim_aspect_embedding],\n",
    "        initializer = tf.random_uniform_initializer(-0.003, 0.003),\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )\n",
    "    bk_va = tf.get_variable(\n",
    "        name = 'aspect_matrix_backward_Va',\n",
    "        shape = [dim_aspect, dim_aspect_embedding],\n",
    "        initializer = tf.random_uniform_initializer(-0.003, 0.003),\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )\n",
    "    wv = tf.get_variable(\n",
    "        name = 'aspect_Wv',\n",
    "        shape = [dim_aspect_embedding * 2, dim_aspect_embedding * 2],\n",
    "        initializer = tf.random_uniform_initializer(-0.003, 0.003),\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )\n",
    "with tf.variable_scope('attention_vars', reuse = tf.AUTO_REUSE):\n",
    "    wh = tf.get_variable(\n",
    "        name = 'M_tanh_Wh',\n",
    "        shape = [dim_lstm * 2, dim_lstm * 2],\n",
    "        initializer = tf.random_uniform_initializer(-0.003, 0.003),\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )\n",
    "    w = tf.get_variable(\n",
    "        name = 'alpha_softmax_W',\n",
    "        shape = [(dim_lstm + dim_aspect_embedding) * 2, 1],\n",
    "        initializer = tf.random_uniform_initializer(-0.003, 0.003),\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )\n",
    "    wp = tf.get_variable(\n",
    "        name = 'hstar_tanh_Wp',\n",
    "        shape = [dim_lstm * 2, dim_lstm * 2],\n",
    "        initializer = tf.random_uniform_initializer(-0.003, 0.003),\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )\n",
    "    wx = tf.get_variable(\n",
    "        name = 'hstar_tanh_Wx',\n",
    "        shape = [dim_lstm * 2, dim_lstm * 2],\n",
    "        initializer = tf.random_uniform_initializer(-0.003, 0.003),\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )\n",
    "with tf.variable_scope('output_softmax_vars', reuse = tf.AUTO_REUSE):\n",
    "    ws = tf.get_variable(\n",
    "        name = 'y_softmax_Ws',\n",
    "        shape = [dim_lstm * 2, dim_polarity],\n",
    "        initializer = tf.random_uniform_initializer(-0.003, 0.003),\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )\n",
    "    bs = tf.get_variable(\n",
    "        name = 'y_softmax_Bs',\n",
    "        shape = [dim_polarity],\n",
    "        initializer = tf.random_uniform_initializer(-0.003, 0.003),\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(reg_eta)\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define lstm model\n",
    "def dynamic_lstm(inputs, seqlen, aspects):\n",
    "#     inputs = tf.nn.dropout(inputs, keep_prob=1.0)\n",
    "    with tf.name_scope('lstm_model'):\n",
    "        # slice the corresponding vai from va\n",
    "        fw_vai = tf.gather(fw_va, aspects) # batch_size x dim_aspect_embedding\n",
    "        bk_vai = tf.gather(bk_va, aspects) # batch_size x dim_aspect_embedding\n",
    "#         # concatenate vai to inputs\n",
    "#         vai_en = [vai for i in range(dim_sentence)]\n",
    "#         vai_en = tf.stack(vai_en, axis = 1) # batch_size x dim_sentence x dim_aspect_embedding\n",
    "#         inputs = tf.concat([inputs, vai_en], 2)\n",
    "        forward_lstm_cell = tf.contrib.rnn.LSTMCell(dim_lstm)\n",
    "        backward_lstm_cell = tf.contrib.rnn.LSTMCell(dim_lstm)\n",
    "        H, states = tf.nn.bidirectional_dynamic_rnn(\n",
    "            forward_lstm_cell,\n",
    "            backward_lstm_cell,\n",
    "            inputs = inputs,\n",
    "            sequence_length = seqlen,\n",
    "            dtype = tf.float32,\n",
    "            scope = 'bilstm'\n",
    "        )\n",
    "        fw, bk = H\n",
    "        bk = tf.reverse_sequence(bk, tf.cast(seqlen, tf.int64), seq_dim=1)\n",
    "        H = tf.concat(H, 2)\n",
    "        size = tf.shape(H)[0]\n",
    "        wv_vai = tf.matmul(tf.concat([fw_vai, bk_vai], 1), wv) # batch_size x (dim_aspect_embedding * 2)\n",
    "        # stacking Wv x Va along sentence length\n",
    "        wv_vai = [wv_vai for i in range(dim_sentence)]\n",
    "        wv_vai_en = tf.stack(wv_vai, axis = 1) # batch_size x dim_sentence x (dim_aspect_embedding * 2)\n",
    "        wv_vai_en = tf.reshape(wv_vai_en, [-1, dim_aspect_embedding * 2]) # (batch_size * dim_sentence) x (dim_aspect_embedding * 2)\n",
    "        H_1 = tf.reshape(H, [-1, dim_lstm * 2]) # (batch_size * dim_sentence) x (dim_lstm * 2)\n",
    "        wh_H = tf.matmul(H_1, wh) # (batch_size * dim_sentence) x (dim_lstm * 2)\n",
    "        # concatenate wh_H and wv_va_En for inputting to tanh\n",
    "        wh_H_wv_vai_en = tf.concat([wh_H, wv_vai_en], 1) # (batch_size * dim_sentence) x [(dim_lstm + dim_aspect_embedding) * 2]\n",
    "        M = tf.tanh(wh_H_wv_vai_en) # (batch_size * dim_sentence) x [(dim_lstm + dim_aspect_embedding) * 2]\n",
    "        alpha = tf.nn.softmax(tf.matmul(M, w)) # (batch_size * dim_sentence)\n",
    "        alpha = tf.reshape(alpha, [-1, 1, dim_sentence]) # batch_size x 1 x dim_sentence\n",
    "        index = tf.range(0, size) * dim_sentence + seqlen - 1 # batch_size\n",
    "        hn = tf.gather(tf.reshape(H, [-1, dim_lstm * 2]), index)  # batch_size x (dim_lstm * 2)\n",
    "        r = tf.reshape(tf.matmul(alpha, H), [-1, dim_lstm * 2]) # batch_size x (dim_lstm * 2)\n",
    "        h_star = tf.tanh(tf.matmul(r, wp) + tf.matmul(hn, wx)) # batch_size x (dim_lstm * 2)\n",
    "        predict = tf.matmul(h_star, ws) + bs # batch x dim_polarity\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# define operations\n",
    "pred = dynamic_lstm(tf.nn.embedding_lookup(u.gloveDict, X), seqlen, aspects)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = pred, labels = y), name = 'op_to_restore')\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "correct = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "# tf saver\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./biatae_save/biatae_batch_train-30\n",
      "[[0.416382   0.4513989  0.13221908]]\n"
     ]
    }
   ],
   "source": [
    "dictionary = {}\n",
    "    with open(dataPath + '%s_filtered.txt' % cf.WORD2VEC_FILE[0:-4], 'r') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = joinWord(values[:-300])\n",
    "            vector = np.array(values[-300:], dtype='float32')\n",
    "            dictionary[word] = vector\n",
    "    f.close()\n",
    "with tf.Session() as sess: \n",
    "    saver = tf.train.import_meta_graph('./biatae_save/biatae_batch_train-30.meta')\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('./biatae_save'))\n",
    "#     graph = tf.get_default_graph()\n",
    "#     loss = graph.get_tensor_by_name(\"op_to_restore:0\")\n",
    "#     test_X, test_y, test_seqlen, test_aspects = u.getData('test')\n",
    "#     train_X, train_y, train_seqlen, train_aspects = u.getData('train')\n",
    "#     fw_va = graph.get_tensor_by_name('aspect_embedding_vars/aspect_matrix_forward_Va:0')\n",
    "#     test = sess.run(loss, feed_dict = {X: train_X, y: train_y, seqlen: train_seqlen, aspects: train_aspects})\n",
    "    testcase = 'The meal is very good!'\n",
    "    words = word_tokenize(testcase)\n",
    "    X = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            idx = \n",
    "    test = sess.run(tf.nn.softmax(pred), {X: np.reshape(test_X[0], [-1, 80]), y: np.reshape(test_y[0], [-1,3]), seqlen: np.reshape(test_seqlen[0], [-1,]), aspects: np.reshape(test_aspects[0],[-1,])})\n",
    "    \n",
    "    print(test)\n",
    "#     test = graph.get_tensor_by_name('aspect_embedding_vars/aspect_Wv:0')\n",
    "#     loss_train = sess.run(loss, feed_dict = {X: train_X, y: train_y, seqlen: train_seqlen, aspects: train_aspects})\n",
    "#     print(loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(test_y[0],[-1,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
